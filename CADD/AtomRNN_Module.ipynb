{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_LOADED_ATOMRNN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a95a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Library 'General (RDKit)'\n",
      "PDB Tools Loaded\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    if GLOBAL_LOADED_PDBREADER: pass\n",
    "except NameError:\n",
    "    %run /home/venkata/python/python_libraries/CADD/PDBReader.ipynb\n",
    "\n",
    "import sys,os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_sequence\n",
    "\n",
    "DEVICE=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa2a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the PDB file as string\n",
    "def readPDBAtomSeq(filename,skipHs=False):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    atypes=[ln[12:16].strip() for ln in lines if (ln[0:4]==\"ATOM\" or ln[0:6]==\"HETATM\") and ((skipHs and ln[12:16].strip()[0]!=\"H\") or (not skipHs))]\n",
    "    return atypes\n",
    "\n",
    "def readPDBFiles(files,N,S,skipHs=False):\n",
    "    rawdata=[]\n",
    "    K=0\n",
    "    L=0\n",
    "    print(\"Target:\",N)\n",
    "    #Skip first 'S', and read 'N' files\n",
    "    file_names = [fn for fn in files]\n",
    "    sorted(file_names)\n",
    "    for f in file_names:\n",
    "        if K<S:\n",
    "            K+=1\n",
    "            continue\n",
    "        if (L+1)%2500==0: print(L+1,f)\n",
    "        rawdata.append([OPENFLAG]+readPDBAtomSeq(folder+\"/\"+f,skipHs=skipHs))\n",
    "        L+=1\n",
    "        if L>=N: break\n",
    "    print(len(rawdata),\"files read\")\n",
    "    return rawdata\n",
    "def samplechar(poss):\n",
    "  posses=np.sum(poss)\n",
    "  if posses<1e-10: return np.random.choice(range(len(poss)))\n",
    "  else:\n",
    "    poss=np.nan_to_num(poss,0.)\n",
    "    poss/=sum(poss)\n",
    "  return np.random.choice(range(len(poss)),p=poss)\n",
    "def buildVocabulary(asets):\n",
    "    global MASK\n",
    "    vocab=set()\n",
    "    for aset in asets:\n",
    "        vocab=set.union(vocab,set(aset))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba56ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some default parameters\n",
    "HIDE=\"\"\n",
    "MASK='\\0'\n",
    "SPLIT=\"~\"\n",
    "OPENFLAG=\"^\"\n",
    "MAXLEN=72\n",
    "BUFFER_SIZE=1000\n",
    "DTYPE_INT=torch.int64\n",
    "\n",
    "datafunc=readPDBAtomSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71679c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeAll(seqlist,keys=dict(),intlevel=torch.int32,strict=False):\n",
    "    maxint=max([keys[k] for k in keys.keys()],default=0)\n",
    "    nextint=maxint+1\n",
    "    ret=[]\n",
    "    for seq in seqlist:\n",
    "        intseq=[]\n",
    "        for el in seq:\n",
    "            if el in keys.keys(): intseq.append(keys[el])\n",
    "            else:\n",
    "                if strict: raise ValueError(\"Key not found: \"+str(el))\n",
    "                intseq.append(nextint)\n",
    "                keys[el]=nextint\n",
    "                nextint+=1\n",
    "        ret.append(torch.tensor(intseq,dtype=intlevel).to(DEVICE))\n",
    "    return ret,keys\n",
    "def rawdataToDataset(rawdata,keys=dict(),intlevel=DTYPE_INT,shuffle=True):\n",
    "    rawdata_encoded,encoded_keys=encodeAll(rawdata,keys,intlevel)\n",
    "    rawdata_encoded=pad_sequence(rawdata_encoded,batch_first=True)\n",
    "    if shuffle:\n",
    "        randord=torch.randperm(len(rawdata_encoded))\n",
    "        return rawdata_encoded[randord],encoded_keys\n",
    "    else:\n",
    "        return rawdata_encoded,encoded_keys\n",
    "    \n",
    "def forcePadding(padded_data,padding,crop=False):\n",
    "    if padding<=padded_data.shape[-1]: return padded_data if not crop else padded_data[:,:padding]\n",
    "    extzeros=torch.zeros((len(padded_data),padding-padded_data.shape[-1])).to(padded_data.device)\n",
    "    return torch.cat((padded_data,extzeros),dim=1)\n",
    "    \n",
    "\n",
    "def loadPDBsForTraining(filenames,N,S,shuffle=True,skipHs=False,keys=dict()): #Quick shortcut\n",
    "    rawdata=readPDBFiles(filenames,N,S,skipHs=skipHs)\n",
    "    return rawdataToDataset(rawdata,shuffle=shuffle,keys=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b71cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED=64\n",
    "RNN_UNITS=512\n",
    "DENSE_UNITS=256\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class RecurrentTemplate(nn.Module):\n",
    "    def __init__(self,vocab,seqlen):\n",
    "        super(RecurrentTemplate,self).__init__()\n",
    "        self.maxlen=seqlen\n",
    "        self.vocabulary=vocab\n",
    "        self.vocabulary[\"\"]=0\n",
    "        self.constructVocabularyInverse()\n",
    "        self.vocab_size=len(self.vocabulary)\n",
    "        self.embedlayer=torch.nn.Embedding(num_embeddings=len(self.vocabulary)+2,embedding_dim=EMBED)\n",
    "        \n",
    "    def constructVocabularyInverse(self):\n",
    "        self.vocabulary_inverse=dict()\n",
    "        for k in self.vocabulary.keys():\n",
    "            self.vocabulary_inverse[self.vocabulary[k]]=k\n",
    "    \n",
    "    def encode(self,lst): #Batched (B,?)\n",
    "        return encodeAll(lst,self.vocabulary)[0]\n",
    "    def decode(self,tens): #Batched\n",
    "        ret=[]\n",
    "        for vec in tens:\n",
    "            nv=[]\n",
    "            for r in vec:\n",
    "                nv.append(self.vocabulary_inverse[int(r)] if r!=0 else MASK)\n",
    "            ret.append(nv)\n",
    "        return ret\n",
    "        \n",
    "\n",
    "class RecurrentLearnerModel(RecurrentTemplate):\n",
    "    def __init__(self,vocab,seqlen,grulayers=2):\n",
    "        super(RecurrentLearnerModel,self).__init__(vocab,seqlen)\n",
    "        self.gru=torch.nn.LSTM(EMBED,RNN_UNITS,num_layers=grulayers,batch_first=True)\n",
    "        self.linear=torch.nn.Linear(RNN_UNITS,DENSE_UNITS)\n",
    "        self.finallayer=torch.nn.Linear(DENSE_UNITS,self.vocab_size+1) #len(vocab) + MASK*1\n",
    "        \n",
    "    def forward(self,x,hidden=None):\n",
    "        #x has shape (B,L)\n",
    "        x=self.embedlayer(x) #Return shape (B,L,E) - where E is embedding dim\n",
    "        outs,hidden=self.gru(x,hidden)\n",
    "        outs=self.linear(outs)\n",
    "        outs=self.finallayer(outs)\n",
    "        return F.softmax(outs,dim=-1),outs,hidden #Outs needed for CrossEntropyLoss\n",
    "    \n",
    "    def predict(self,x,hidden=None,limit_k=-1):\n",
    "        #x has shape (B,L)\n",
    "        #Output has shape (B,1) - with one prediction at each place\n",
    "        #limit_k limits outputs to top 'k' most probable results\n",
    "        preds,_,hidden=self.forward(x,hidden)\n",
    "        if limit_k<=0: limit_k = preds.shape[-1]\n",
    "        preds,inds=preds.topk(limit_k)\n",
    "        #print(preds.shape,inds.shape,torch.sum(preds,dim=-1)[:,:,np.newaxis].shape,preds[0].multinomial(1,replacement=False).shape)\n",
    "        preds=preds/torch.sum(preds,dim=-1)[:,:,np.newaxis]\n",
    "        predret=[]\n",
    "        for i,pv in enumerate(preds):\n",
    "            selind=torch.squeeze(pv.multinomial(1,replacement=False),dim=-1) # (seqlen) - list of seqlen indices chosen\n",
    "            truinds=torch.stack([v[selind[j]] for j,v in enumerate(inds[i])])\n",
    "            predret.append(truinds)\n",
    "        predmul=torch.stack(predret)\n",
    "        #predmul=torch.stack([torch.squeeze(inds[i,]) for i,pv in enumerate(preds)])\n",
    "        return predmul,hidden\n",
    "    \n",
    "    def generate(self,num,maxlen,limit_k=-1,use_hidden=None):\n",
    "        hidden=use_hidden\n",
    "        base_start=[['^']]*num\n",
    "        inseq=self.encode(base_start)\n",
    "        inseq=torch.stack(inseq)\n",
    "        curseq=inseq\n",
    "        ended=torch.zeros(len(base_start),dtype=torch.bool).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            while curseq.shape[1]<maxlen and not torch.all(ended):\n",
    "                res,hidden=self.predict(inseq,hidden,limit_k)\n",
    "                vals=torch.squeeze(res,dim=-1)==MASK\n",
    "                ended=ended | vals\n",
    "                curseq=torch.cat((curseq,res),dim=1)\n",
    "        return self.decode(curseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbea6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "import abc\n",
    "class GenericModelTrainer(metaclass=abc.ABCMeta):\n",
    "    def __init__(self,model,optimizer_type,learnrate,lossfn):\n",
    "        self.model=model\n",
    "        self.optimizer = optimizer_type(self.model.parameters(),lr=learnrate)\n",
    "        self.lossfn=lossfn\n",
    "    \n",
    "    def useModel(self,inputs): return self.model(inputs)\n",
    "    def evaluateModel(self,inputs,labels,batch_size=BATCH_SIZE,metrics=[],print_out=False):\n",
    "        metric_profile=dict()\n",
    "        for met in metrics: metric_profile[str(met)] = []\n",
    "        with torch.no_grad():\n",
    "            net_loss=[]\n",
    "            for idx in range(len(inputs)//BATCH_SIZE):\n",
    "                self.optimizer.zero_grad()\n",
    "                inpdata=inputs[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE]\n",
    "                outdata=labels[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE]\n",
    "\n",
    "                sample = self.useModel(inpdata)\n",
    "                loss=self.lossfn(sample,outdata)\n",
    "                for met in metrics:\n",
    "                    metx=met(sample,outdata)\n",
    "                    metric_profile[str(met)].append(metx.item())\n",
    "                net_loss.append(loss.item())\n",
    "            if len(metrics):\n",
    "                if print_out: print(\"Metrics:\",end=\" \")\n",
    "                for met in metrics:\n",
    "                    meanmet=np.mean(metric_profile[str(met)])\n",
    "                    metric_profile[str(met)]=meanmet\n",
    "                    if print_out: print(str(met),meanmet,end=\", \")\n",
    "                if print_out: print()\n",
    "        if len(metrics): return np.mean(net_loss)\n",
    "        else: return np.mean(net_loss),metric_profile\n",
    "                \n",
    "        \n",
    "    \n",
    "    def trainModel(self,inputs,labels,epochs,batch_size=BATCH_SIZE,metrics=[],save=False,save_every=5,save_path=\"model_training.\"):\n",
    "        loss_profile=[]\n",
    "        loss_metrics=dict()\n",
    "        for met in metrics: loss_metrics[str(met)] = []\n",
    "        EPOCHS=epochs\n",
    "        SAVE=5\n",
    "        SAVE_PATH = save_path # \"saved_models/Seqlearner_Amac_PT_weightsonly\"\n",
    "        PROGRESS_STEP = (len(inputs)//BATCH_SIZE)//72\n",
    "        for ep in range(EPOCHS):\n",
    "            net_loss=[]\n",
    "            timestart=time.time()\n",
    "            for met in metrics: loss_metrics[str(met)].append([])\n",
    "            print(\"Epoch\",ep+1,\"[\",end=\"\")\n",
    "            for idx in range(len(inputs)//BATCH_SIZE):\n",
    "                self.optimizer.zero_grad()\n",
    "                inpdata=inputs[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE]\n",
    "                outdata=labels[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE]\n",
    "                \n",
    "                sample = self.useModel(inpdata)\n",
    "                loss=self.lossfn(sample,outdata)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                for met in metrics:\n",
    "                    metx=met(sample,outdata)\n",
    "                    loss_metrics[str(met)][-1].append(metx.item())\n",
    "                \n",
    "                net_loss.append(loss.item())\n",
    "                if idx%PROGRESS_STEP==0: print(\"=\",end=\"\",flush=True)\n",
    "            timeend=time.time()\n",
    "            print(\"]\",flush=True)\n",
    "            loss_profile.append(np.mean(net_loss))\n",
    "            print(\"Epoch\",ep+1,\"with loss:\",loss_profile[-1],\"took time:\",timeend-timestart)\n",
    "            if len(metrics):\n",
    "                print(\"Metrics:\",end=\" \")\n",
    "                for met in metrics:\n",
    "                    meanmet=np.mean(loss_metrics[str(met)][-1])\n",
    "                    loss_metrics[str(met)][-1]=meanmet\n",
    "                    print(str(met),meanmet,end=\", \")\n",
    "                print()\n",
    "            if save and ep%SAVE==0: torch.save(mymodel,SAVE_PATH) #torch.save(mymodel.state_dict(),SAVE_PATH)\n",
    "        if save: torch.save(mymodel,SAVE_PATH)\n",
    "        print(\"Completed\")\n",
    "        if len(metrics): return loss_profile,loss_metrics\n",
    "        else: return loss_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(RecurrentTemplate):\n",
    "    def __init__(self,vocab,seqlen,classes,rnn_depth=1,deep_model=None):\n",
    "        super(RecurrentClassifier,self).__init__(vocab,seqlen)\n",
    "        self.num_classes=classes\n",
    "        self.gru=torch.nn.LSTM(EMBED,RNN_UNITS,num_layers=rnn_depth,batch_first=True)\n",
    "        if deep_model is None: deep_model = torch.nn.Linear(RNN_UNITS,2)\n",
    "        self.linear = deep_model\n",
    "        \n",
    "    def forward(self,x,hidden=None):\n",
    "        #x has shape (B,L)\n",
    "        x=self.embedlayer(x) #Return shape (B,L,E) - where E is embedding dim\n",
    "        outs,hidden=self.gru(x,hidden)\n",
    "        outs=self.linear(outs)\n",
    "        return F.softmax(outs,dim=-1),outs,hidden #Outs needed for CrossEntropyLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_metric_accuracy(preds,trues):\n",
    "    preds=torch.max(preds,dim=-1).indices.detach()\n",
    "    return torch.sum(preds==trues)/len(preds)\n",
    "def constantScore(inp): return 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if GLOBAL_LOADED_REINFORCE: pass\n",
    "except NameError:\n",
    "    %run /home/venkata/python/python_libraries/python/Reinforcement_Module.ipynb\n",
    "'''\n",
    "    Molecule Generation Game: The goal of this game is to generate molecules that satisfy certain criteria (such as net charge, size, etc.)\n",
    "    The generator returns one of 'N' characters (decided apriori), and the generation ends when the model generates a '0'\n",
    "'''\n",
    "class RNNMolGenerationEnvironment(GenericDiscreteEnvironmentExtension,metaclass=abc.ABCMeta):\n",
    "    def __init__(self,num_tokens,max_len,end_on=0,startWith=None,score_fn=constantScore,fail_fn=constantScore,score_kws=dict(),fail_kws=dict()):\n",
    "        super(RNNMolGenerationEnvironment,self).__init__((1,),getBoundedObservables((1,),0,num_tokens+1),num_tokens+1,False)\n",
    "        self.endtoken=end_on\n",
    "        self.num_tokens = num_tokens\n",
    "        self.maxlen = max_len\n",
    "        self.scoring = score_fn\n",
    "        self.failing = fail_fn\n",
    "        self.fail_kws=fail_kws\n",
    "        if startWith is not None: self.start=np.array(startWith,dtype=np.int64) #Need to implement\n",
    "        else: self.start = np.zeros((1,),dtype=np.int64)\n",
    "        \n",
    "        self.state=self.start\n",
    "        self.ended=False\n",
    "        self.genseq=self.state\n",
    "        \n",
    "        #Correcting \"Action Space\" for batching\n",
    "        #self.action_space = getBoundedObservables((1,),0,num_tokens+1)\n",
    "        self.action_space = IntegerActions(0,num_tokens+1)\n",
    "        \n",
    "        self.score_kws=score_kws\n",
    "    \n",
    "    def isTerminalState(self): return self.genseq.shape[-1]>self.maxlen or self.ended\n",
    "    def getObservation(self): return self.state[np.newaxis,:]\n",
    "    \n",
    "    def getReward(self):\n",
    "        if self.isTerminalState(): return (self.scoring(self.genseq,**self.score_kws) if self.genseq.shape[-1]<=self.maxlen else torch.tensor(self.failing(self.genseq,**self.fail_kws)*(~self.ended),dtype=torch.float32))\n",
    "        else: return 0.\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state=self.start\n",
    "        self.genseq=self.state\n",
    "        self.ended=False\n",
    "        return self.getObservation()\n",
    "    \n",
    "    def resolveAction(self,act): #act is a set of batched actions (B,1) like (5,6,2,2,2,4,2,5,0 ... ) as a NUMPY array\n",
    "        self.state=np.array([act])\n",
    "        self.genseq=np.append(self.genseq,act)\n",
    "        self.ended=(act==0)\n",
    "        return self.isTerminalState()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
