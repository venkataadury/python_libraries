{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac46a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_LOADED_REINFORCE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c28bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,time\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import Env,spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import abc\n",
    "from collections.abc import Iterable\n",
    "import copy\n",
    "\n",
    "IS_IPYTHON = 'inline' in matplotlib.get_backend()\n",
    "if IS_IPYTHON:\n",
    "    from IPython import display\n",
    "    #from google.colab.patches import cv2_imshow\n",
    "\n",
    "DEVICE=torch.device(\"cuda\")\n",
    "BATCH_SIZE=32\n",
    "STEP_LIM=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0fbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoundedObservables(shp,lowv,highv,set_dtype=np.float32):\n",
    "    return spaces.Box(low = np.ones(shp)*lowv, high = np.ones(shp)*highv, dtype = set_dtype)\n",
    "class GenericEnvironmentExtension(Env,metaclass=abc.ABCMeta):\n",
    "    def __init__(self,observe_shape,observe_space,action_space,renderable=False):\n",
    "        super(GenericEnvironmentExtension, self).__init__()\n",
    "        \n",
    "        # Define a 2-D observation space\n",
    "        self.observation_shape = observe_shape\n",
    "        self.observation_space = observe_space\n",
    "        self.ep_return=0\n",
    "    \n",
    "        \n",
    "        # Define an action space ranging from 0 to 4\n",
    "        self.action_space = action_space #spaces.Discrete(6,)\n",
    "        \n",
    "        self.renderable=renderable\n",
    "                        \n",
    "        # Create a canvas to render the environment images upon \n",
    "        if renderable: self.canvas = np.ones(self.observation_shape) * 1\n",
    "        \n",
    "    \n",
    "    def resetEpisodicReturn(self): self.ep_return=0\n",
    "    def getEpisodicReturn(self): return self.ep_return\n",
    "    def addEpisodicReward(self,rew): self.ep_return+=rew\n",
    "        \n",
    "    \n",
    "    def draw_elements_on_canvas(self,elements):\n",
    "        if not self.renderable: return False\n",
    "        self.canvas = np.ones(self.observation_shape) * 1\n",
    "\n",
    "        # Draw the heliopter on canvas\n",
    "        for elem in elements:\n",
    "            if elem.icon is None: print(\"None icon:\",elem.name)\n",
    "            elem_shape = elem.icon.shape\n",
    "            x,y = elem.x, elem.y\n",
    "            self.canvas[y : y + elem_shape[1], x:x + elem_shape[0]] = elem.icon\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def render(self, mode = \"human\"):\n",
    "        assert mode in [\"human\", \"rgb_array\"], \"Invalid mode, must be either \\\"human\\\" or \\\"rgb_array\\\"\"\n",
    "        if mode == \"human\":\n",
    "            print(\"Human Mode is not supported!\")\n",
    "            #cv2.imshow(\"Window\",self.canvas)\n",
    "            #cv2.waitKey(10)\n",
    "\n",
    "        elif mode == \"rgb_array\":\n",
    "            return self.canvas\n",
    "    def close(self): pass #cv2.destroyAllWindows() #Overridable if needed\n",
    "    \n",
    "    def checkIfActionAllowed(self,action): return self.action_space.contains(action)\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def reset(self): pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def getObservation(self): pass\n",
    "    \n",
    "    def step(self,action): #Returns observation (after step), reward (after step), done (Y/N)?, optional extras (in case the function is overridden)\n",
    "        done = False\n",
    "        # Assert that it is a valid action\n",
    "        assert self.checkIfActionAllowed(action), \"Invalid Action\"\n",
    "        \n",
    "        done = done or self.beforeAction(action)\n",
    "        done = done or self.resolveAction(action)\n",
    "        done = done or self.afterAction(action)\n",
    "        \n",
    "        reward=self.getReward()\n",
    "        self.addEpisodicReward(reward)\n",
    "        \n",
    "        return self.getObservation(), reward, done, []\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def resolveAction(self,action): pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def getReward(self): pass\n",
    "    \n",
    "    def beforeAction(self,action): return False\n",
    "    def afterAction(self,action): return False\n",
    "        \n",
    "\n",
    "class GenericDiscreteEnvironmentExtension(GenericEnvironmentExtension,metaclass=abc.ABCMeta):\n",
    "    def __init__(self,observe_shape,observe_space,nact,renderable=False):\n",
    "        super(GenericDiscreteEnvironmentExtension,self).__init__(observe_shape,observe_space,spaces.Discrete(nact,),renderable=renderable)\n",
    "        self.action_meanings=dict()\n",
    "    \n",
    "    def getActionMeanings(self): return self.action_meanings\n",
    "    def addActionMeanings(self,acts,means): self.addActionMeaning(acts,means)\n",
    "    def addActionMeaning(self,acts,means):\n",
    "        if not isinstance(acts,Iterable):\n",
    "            acts=[acts]\n",
    "            means=[means]\n",
    "        for i,av in enumerate(acts):\n",
    "            self.action_meanings[av]=means[i]\n",
    "    \n",
    "    def checkIfActionAllowed(self,action): return self.action_space.contains(action)\n",
    "    def getActionCount(self): return self.action_space.n\n",
    "    def getObservationShape(self): return self.observation_shape\n",
    "class IntegerActions:\n",
    "    def __init__(self,minv,maxv):\n",
    "        self.minv=minv\n",
    "        self.maxv=maxv\n",
    "    \n",
    "    def contains(self,val): return self.minv <= val and self.maxv>=val\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class GenericTorchReinforcementModel(nn.Module):\n",
    "    def __init__(self,inshape,outshape):\n",
    "        super(GenericTorchModel,self).__init__()\n",
    "        self.inshape=inshape\n",
    "        self.outshape=outshape\n",
    "    \n",
    "    def transform(self,x): return x #Overridable\n",
    "    def resetModel(self): pass #Overridable\n",
    "\n",
    "class SimpleModel(GenericTorchReinforcementModel): #(D)QN (Q-value predictor) for this problem\n",
    "    def __init__(self,inshape,outshape,actcount,activation=F.elu):\n",
    "        super(SimpleModel,self).__init__(inshape,outshape)\n",
    "        self.acts=actcount\n",
    "        self.activation=activation\n",
    "        \n",
    "        #Layers\n",
    "        self.layer=nn.Linear(np.prod(inshape),np.prod(outshape))\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        if type(inp)!=torch.Tensor: inp=torch.tensor(inp,dtype=torch.float32).to(DEVICE)\n",
    "        else: inp=inp.to(DEVICE)\n",
    "        return self.activation(self.layer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521d209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self,model,nacts):\n",
    "        self.num_actions=nacts\n",
    "        self.basemodel=model\n",
    "        self.action_weights=nn.Linear(np.prod(model.outshape),nacts) # Probability over action space\n",
    "        self.action_values=nn.Linear(np.prod(model.outshape),1) # One value prediction for the state\n",
    "        self.refreshParameterList()\n",
    "    \n",
    "    def refreshParameterList(self):\n",
    "        self.parameterlist=[]\n",
    "        for param in self.basemodel.parameters(): self.parameterlist.append(param)\n",
    "        for param in self.action_values.parameters(): self.parameterlist.append(param)\n",
    "        for param in self.action_weights.parameters(): self.parameterlist.append(param)\n",
    "    \n",
    "    def getActionAndValue(self,obs):\n",
    "        x=self.basemodel(obs)\n",
    "        actions = F.softmax(self.action_weights(x),dim=-1)\n",
    "        value = self.action_values(x)\n",
    "        return actions,value\n",
    "    \n",
    "    def parameters(self): return nn.ParameterList(self.parameterlist)\n",
    "    def to(self,dev):\n",
    "        self.action_values = self.action_values.to(dev)\n",
    "        self.action_weights = self.action_weights.to(dev)\n",
    "        self.basemodel = self.basemodel.to(dev)\n",
    "        return self\n",
    "    \n",
    "    def selectAction(self,obs,eps=0.,training=False,return_value=False,return_distribution=False,return_logits=False):\n",
    "        if return_logits: return_distribution=True\n",
    "        with torch.no_grad() if not training else torch.enable_grad():\n",
    "            acts,val=self.getActionAndValue(obs)\n",
    "            # create a categorical distribution over the list of probabilities of actions\n",
    "            m = Categorical(acts)\n",
    "            # and sample an action using the distribution\n",
    "            action = m.sample()\n",
    "        if return_value:\n",
    "            if return_distribution: return action,val,(acts if return_logits else m)\n",
    "            else: return action,val\n",
    "        else: return ((action,(acts if return_logits else m)) if return_distribution else action)\n",
    "\n",
    "class ActorCriticTrainer: #Episode-wise trainer\n",
    "    def __init__(self,env,agent,optimizer,learnrate=1e-3):\n",
    "        self.env=env\n",
    "        self.agent=agent\n",
    "        self.optimizer=optimizer(self.agent.parameters(),lr=learnrate) #Reconstruct trainer object to reset optimizer\n",
    "        \n",
    "        self.resetMemory()\n",
    "    \n",
    "    def resetMemory(self):\n",
    "        self.saved_actions=[]\n",
    "        self.rewards=[]\n",
    "        \n",
    "    def supervisedTraining(inputs,labels,lossfn,batch_size=BATCH_SIZE,epochs=1): #Both are torch tensors\n",
    "        for ep in range(epochs):\n",
    "            losses=[]\n",
    "            for i in range(inputs.shape[0]//BATCH_SIZE):\n",
    "                self.optimizer.zero_grad()\n",
    "                curins=inputs[i*BATCH_SIZE,(i+1)*BATCH_SIZE]\n",
    "                curouts=labels[i*BATCH_SIZE,(i+1)*BATCH_SIZE]\n",
    "                modouts=self.agent.action_weights(self.agent.basemodel()) # Note: Softmax NOT APPLIED on purpose\n",
    "                loss=lossfn(modouts,curouts)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "            print(\"Epoch\",ep,\"with loss\",np.mean(losses))\n",
    "        print(\"Completed Training\")\n",
    "        \n",
    "    \n",
    "    def playEpisode(self,steplim=STEP_LIM,printevery=250,remember=True,batched_action=False):\n",
    "        obs=self.env.reset()\n",
    "        total_reward=0\n",
    "        done = False\n",
    "        for t in count(1):\n",
    "            # \"training=True\" enables gradients (torch autograd)\n",
    "            act,estval,m=self.agent.selectAction(obs,return_value=True,return_distribution=True,training=True)\n",
    "            if not batched_action: obs, reward, done, _ = env.step(act.item())\n",
    "            else: obs, reward, done, _ = env.step(act.detach().cpu().view(-1,1))\n",
    "            total_reward+=reward\n",
    "            \n",
    "            if remember:\n",
    "                self.saved_actions.append((m.log_prob(act),estval))\n",
    "                self.rewards.append(reward)\n",
    "            \n",
    "            if done or t>steplim: break\n",
    "            if printevery>0 and (t+1)%printevery==0: print(t,end=\" \",flush=True)\n",
    "        if batched_action:\n",
    "            total_reward=torch.mean(total_reward).item()\n",
    "            try: pass\n",
    "            except:\n",
    "                print(\"WARN: Total reward is not a tensor! This might mean that the environment is badly configured to return numeric/non-torch tensors as rewards!\\nTrying to directly interpret as a number\")\n",
    "                print(\"Object is:\",total_reward)\n",
    "                try: total_reward=float(total_reward)\n",
    "                except:\n",
    "                    print(\"Total reward cannot be converted to float.\")\n",
    "                    print(\"Trying to use numpy instead!\")\n",
    "                    total_reward=float(np.mean(total_reward)) \n",
    "                    \n",
    "        return total_reward,copy.deepcopy(env)\n",
    "    \n",
    "    def learnFromMemory(self,clear=True,value_loss=F.smooth_l1_loss,compute_only_loss=False,batched_action=False,batch_aggregation=torch.mean,agg_kws=dict()):\n",
    "        R = 0\n",
    "        saved_actions = self.saved_actions\n",
    "        policy_losses = [] # list to save actor (policy) loss\n",
    "        value_losses = [] # list to save critic (value) loss\n",
    "        returns = [] # list to save the true values\n",
    "\n",
    "        # calculate the true value using rewards returned from the environment\n",
    "        for r in self.rewards[::-1]:\n",
    "            # calculate the discounted value\n",
    "            R = r + GAMMA * R\n",
    "            returns.insert(0, R)\n",
    "        if not batched_action: print(\"Found R=\",R)\n",
    "\n",
    "        try: #Assume no batching (`returns` is a list of numbers )\n",
    "            returns = torch.tensor(returns,dtype=torch.float32,device=DEVICE)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + EPS_ZERO)\n",
    "        except: # With batching (`returns` is a list of tensors)?\n",
    "            returns=torch.stack(returns).to(DEVICE)\n",
    "            returns = (returns-returns.mean(dim=1).view(-1,1))/(returns.std(dim=1).view(-1,1)+EPS_ZERO)\n",
    "\n",
    "        for (log_prob, value), R in zip(saved_actions, returns):\n",
    "            if batched_action and batch_aggregation is not None:\n",
    "                advantage = batch_aggregation(R - value.detach(),**agg_kws).item()\n",
    "            else:\n",
    "                advantage = R - value.item()\n",
    "\n",
    "            # calculate actor (policy) loss \n",
    "            policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "            # calculate critic (value) loss using L1 smooth loss\n",
    "            if batched_action: value_losses.append(value_loss(value, R))\n",
    "            else: value_losses.append(value_loss(value, torch.tensor([R])))\n",
    "\n",
    "        # reset gradients\n",
    "        if not compute_only_loss: self.optimizer.zero_grad()\n",
    "\n",
    "        # sum up all the values of policy_losses and value_losses\n",
    "        loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "        if clear:\n",
    "            self.resetMemory()\n",
    "            self.agent.basemodel.resetModel()\n",
    "            \n",
    "        if compute_only_loss: return loss\n",
    "        \n",
    "        # perform backprop\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.agent.parameters(), CLIP_NORMS)\n",
    "        self.optimizer.step() #TODO Enable this to train\n",
    "\n",
    "        # reset rewards and action buffer\n",
    "        \n",
    "        return loss\n",
    "            \n",
    "    \n",
    "    def trainEpisodewise(self,running=20,steplim=STEP_LIM,episode_lim=-1,reward_high=None,batched_action=False,batch_aggregation=None,agg_kws=dict(),save_every=None,save_name=\"agent.pt\",**kwargs):\n",
    "        if save_every is None: save_every=running\n",
    "        running_reward=0\n",
    "        ratio=1/running\n",
    "        sat=False\n",
    "        for epno in count(1):\n",
    "            print(\"Episode\",epno)\n",
    "            self.resetMemory()\n",
    "            total_rew,lastenv=self.playEpisode(steplim=steplim,remember=True,batched_action=batched_action,**kwargs)\n",
    "            print(\"Played\")\n",
    "            self.learnFromMemory(clear=True,batched_action=batched_action,batch_aggregation=batch_aggregation,agg_kws=agg_kws)\n",
    "            \n",
    "            if episode_lim>0 and epno>=episode_lim: break\n",
    "            running_reward=(1-ratio)*running_reward+total_rew\n",
    "            print(\"Running Reward:\",running_reward)\n",
    "            \n",
    "            if reward_high is not None:\n",
    "                #if reward_high<running_reward and epno>running:\n",
    "                if reward_high<running_reward and epno>running: # Treating rewards as loss?\n",
    "                    print(\"Target reward reached!\")\n",
    "                    sat=True\n",
    "                    break\n",
    "            if save_every>0 and epno%save_every==0:\n",
    "                torch.save(self.agent,save_name)\n",
    "                print(\"Saved model at '\"+str(save_name)+\"'\")\n",
    "        torch.save(self.agent,save_name)\n",
    "        return sat\n",
    "    \n",
    "    def trainBatchwise(self,batch_size,running=20,steplim=STEP_LIM,batch_lim=-1,reward_high=None,**kwargs):\n",
    "        self.lossmem=[]\n",
    "        running_reward=0\n",
    "        ratio=1/running\n",
    "        sat=False\n",
    "        batchK=0\n",
    "        for epno in count(1):\n",
    "            print(\"Episode\",epno)\n",
    "            self.resetMemory()\n",
    "            total_rew,lastenv=self.playEpisode(steplim=steplim,remember=True,**kwargs)\n",
    "            print(\"Played\")\n",
    "            myloss=self.learnFromMemory(clear=True,compute_only_loss=True)\n",
    "            self.lossmem.append(myloss)\n",
    "            \n",
    "            if len(self.lossmem) >= batch_size:\n",
    "                self.optimizer.zero_grad()\n",
    "                final_loss=torch.sum(torch.stack(self.lossmem))\n",
    "                final_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.lossmem=[]\n",
    "                batchK+=1\n",
    "            \n",
    "            if batch_lim>0 and batchK>=batch_lim: break\n",
    "            running_reward=(1-ratio)*running_reward+total_rew\n",
    "            print(\"Running Reward:\",running_reward)\n",
    "            \n",
    "            if reward_high is not None:\n",
    "                if reward_high<running_reward:\n",
    "                    print(\"Target reward reached!\")\n",
    "                    sat=True\n",
    "                    break\n",
    "        return sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c863bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineParameters(parsets):\n",
    "    ret=[]\n",
    "    for paramlist in parsets:\n",
    "        for pobj in paramlist: ret.append(pobj)\n",
    "    return torch.nn.ParameterList(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reinforcement Module Loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
